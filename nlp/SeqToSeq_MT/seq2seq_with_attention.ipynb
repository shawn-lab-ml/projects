{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z44j_0shMWGT"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INoT65sx9UjQ"
      },
      "source": [
        "## Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHwP-EVcNa9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10ec658-fcb4-47d1-8fd8-8adb65c81587"
      },
      "source": [
        "# run code then restart runtime\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download de_core_news_md\n",
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 66.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051304 sha256=114f267ea51565db4715506f1665a54b909b3ebcb200d26403ead35ba7a3979b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kot3j__5/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "Collecting de_core_news_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.2.5/de_core_news_md-2.2.5.tar.gz (224.6MB)\n",
            "\u001b[K     |████████████████████████████████| 224.6MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_md==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_md==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-md\n",
            "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-md: filename=de_core_news_md-2.2.5-cp36-none-any.whl size=228399479 sha256=2a6eb0a90413ad76cc9f31442dd7bc5b97d657a919b207d962585197f9ac0738\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-09lz0ly_/wheels/41/60/41/81898870259d7c19fe8f9e46a537611c939f0c425eee2e1785\n",
            "Successfully built de-core-news-md\n",
            "Installing collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_md')\n",
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8hkCu199XKe"
      },
      "source": [
        "## Librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "altv54EUD6Nz"
      },
      "source": [
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "# Usual\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hzDbrDDtwG"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8inUF3nxDtNU"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "\n",
        "    # Load german tokenizer\n",
        "    #spacy_ger = spacy.load(\"de_core_news_md\")\n",
        "\n",
        "    # tokenizing the sentences using spacy\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # print(tokens)\n",
        "\n",
        "    # Adding sos and eos token\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    # string to index\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor of size (len(text_to_indices), 1) for pytorch\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        outputs_encoder,hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        #disabling gradient calculation bc no use in inference\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, outputs_encoder, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "\n",
        "def evaluation_fn(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"----------- Saving checkpoint -----------\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"----------- Saving checkpoint -----------\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BebvPUOAAqGF"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRLKLkyyAull"
      },
      "source": [
        "class Config:\r\n",
        "  save_filename = \"checkpoint.pth.tar\"\r\n",
        "  load_model = False\r\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "  encoder_embedding_size = 300\r\n",
        "  decoder_embedding_size = 300\r\n",
        "  hidden_size = 1024\r\n",
        "  num_layers = 4\r\n",
        "  enc_dropout = 0.0\r\n",
        "  dec_dropout = 0.0\r\n",
        "  num_epochs = 30\r\n",
        "  learning_rate = 1e-4\r\n",
        "  batch_size = 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWsyd8RPSlKb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A640YJPdSm0D"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTf3OOzFjR1q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU1oCfLpSmNq"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, pdropout):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.dropout = nn.Dropout(pdropout)\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size,  num_layers, bidirectional = True)\n",
        "\n",
        "    self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "    self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape (seq_length, batch_size)\n",
        "    # embedding shape (seq_length, batch_size, embedding_size)\n",
        "    # hidden / cell shape (num_layers *num_directions, batch_size, hidden_size) => (2*num_layers, batch_size, hidden_size)\n",
        "    # hidden / cell shape after linear layer: (1, batch_size, hidden_size*2)\n",
        "    # encoder states shape: (seq_length, batch_size, hidden_size*num_layers)\n",
        "\n",
        "    # Word Embedding => building word vectors using dataset\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Using Bi-directional LSTMs on Word Vectors\n",
        "    encoder_states, (hidden,cell) = self.rnn(embedding)\n",
        "\n",
        "    # The hidden layer going forward is hidden[0:1] and the one going backward is hidden[1:2]\n",
        "    # Use both the forward and backward cell / hidden layer and run it through a Linear layer\n",
        "    # The decoder layer is not bidirectional\n",
        "\n",
        "\n",
        "\n",
        "    #hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "    #cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "    hidden = self.fc_hidden(torch.cat((hidden[0:4], hidden[4:8]), dim=2))\n",
        "    cell = self.fc_cell(torch.cat((cell[0:4], cell[4:8]), dim=2))\n",
        "    #encoder_states = torch.cat((encoder_states[:,:,0:1024], encoder_states[:,:,1024:]), dim=0)\n",
        "\n",
        "    #print(encoder_states.shape)\n",
        "\n",
        "    return encoder_states, hidden, cell\n",
        "  \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54DAp1cOSxTt"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jsduTwpSwXa"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers, pdropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(pdropout)\n",
        "\n",
        "    self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size,  num_layers)\n",
        "    \n",
        "    self.energy = nn.Linear(hidden_size*3, 1)\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "  def forward(self, x, encoder_states, hidden, cell):\n",
        "    # Shape of x: (batch_size) as we are decoding one word at a time \n",
        "    # but we need (1, batch_size) so we unsqueeze the input\n",
        "    # embedding shape (1, batch_size, embedding_size)\n",
        "\n",
        "    #hidden_reshape shape (sequence_length, batch_size, hidden_size*2)\n",
        "    # energy: (sequence_length, batch_size, 1)\n",
        "    # attention shape (sequence_length, batch_size, 1)\n",
        "\n",
        "    # context_vector shape: (1, batch_size, hidden_size*2)\n",
        "    # rnn_input shape: (1, batch_size, hidden_size*2 + embedding_size)\n",
        "    # outputs shape (1, batch_size, hidden_size)\n",
        "    # predictions shape: (batch_size, hidden_size)\n",
        "\n",
        "    x = x.unsqueeze(0)\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    sequence_length = encoder_states.shape[0]\n",
        "    hidden_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "\n",
        "    #print(encoder_states[:,:,1536:2048].shape)\n",
        "\n",
        "    encoder_states = encoder_states.repeat(self.num_layers, 1, 1)\n",
        "\n",
        "    #print(hidden_reshaped.shape, encoder_states.shape)\n",
        "\n",
        "    energy = self.relu(self.energy(torch.cat((hidden_reshaped, encoder_states), dim=2)))\n",
        "    attention = self.softmax(energy)\n",
        "\n",
        "    # we want context_vector: (1, batch_size, hidden_size*2), i.e knl\n",
        "    # attention: (sequence_length, batch_size, 1), snk\n",
        "    # encoder_states: (sequence_length, batch_size, hidden_size*2), snl\n",
        "    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
        "\n",
        "    rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "    outputs, (hidden,cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "\n",
        "    predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "    return predictions, hidden, cell"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZXr87nyS5jO"
      },
      "source": [
        "## seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzGEf-DxS2aI"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio = 0.5):\n",
        "\n",
        "    # source shape: (source_length, batch_size)\n",
        "    # target shape: (target_length, batch_size)\n",
        "    \n",
        "    batch_size = source.shape[1]\n",
        "    target_len =target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "  \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(Config.device)\n",
        "    encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "    # Grab start token\n",
        "    x = target[0]\n",
        "\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "      outputs[t] = output\n",
        "\n",
        "      best_guess = output.argmax(1)\n",
        "\n",
        "      # if random < teacher_force_ratio then the target word is used\n",
        "      # else, we use the output from the model\n",
        "      # This allows the model to have similar inputs at both training and testing time\n",
        "      # Testing time is similar to having teacher_force_ratio set to 0\n",
        "\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "    \n",
        "    return outputs\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RbGcvdUHKIw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4P5HtJmP2sp"
      },
      "source": [
        "spacy_ger = spacy.load('de_core_news_md')\r\n",
        "spacy_en = spacy.load('en_core_web_md')\r\n",
        "\r\n",
        "def tokenizer_ger(text):\r\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenizer_en(text):\r\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "german = Field(tokenize = tokenizer_ger, lower = True, init_token='<sos>', eos_token='<eos>')\r\n",
        "\r\n",
        "english = Field(tokenize = tokenizer_en, lower = True,init_token='<sos>', eos_token='<eos>')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "649Vzm1Mm85E",
        "outputId": "8aca708a-ce88-43be-fa65-a317e0ffd5c9"
      },
      "source": [
        "train_data, validation_data, test_data = Multi30k.splits(exts=(\".de\", \".en\"),fields=(german,english))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 313kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 91.9kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 86.0kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXDJOXYPV4a"
      },
      "source": [
        "def train_fn(train_data, validation_data, test_data):\r\n",
        "\r\n",
        "  #building vocabulary (tokenizing my training data)\r\n",
        "  german.build_vocab(train_data, max_size=10000, min_freq=2)\r\n",
        "  english.build_vocab(train_data, max_size=10000, min_freq=2)\r\n",
        "\r\n",
        "  input_size_encoder = len(german.vocab)\r\n",
        "  input_size_decoder = len(english.vocab)\r\n",
        "  output_size = len(english.vocab)\r\n",
        "\r\n",
        "  train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "  (train_data, validation_data, test_data),\r\n",
        "  batch_size = Config.batch_size,\r\n",
        "  sort_within_batch = True,\r\n",
        "  sort_key = lambda x: len(x.src),\r\n",
        "  device=Config.device)\r\n",
        "\r\n",
        "  encoder = Encoder(input_size_encoder, Config.encoder_embedding_size, Config.hidden_size, Config.num_layers, Config.enc_dropout).to(Config.device)\r\n",
        "  decoder = Decoder(input_size_decoder, Config.decoder_embedding_size, Config.hidden_size, output_size, Config.num_layers, Config.dec_dropout).to(Config.device)\r\n",
        "  model = Seq2Seq(encoder, decoder).to(Config.device)\r\n",
        "\r\n",
        "  optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\r\n",
        "  steps_per_epoch = len(train_data)\r\n",
        "\r\n",
        "  # adding padding but don't want to pay anything for it in the cost function\r\n",
        "  pad_idx = english.vocab.stoi['<pad>']\r\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\r\n",
        "\r\n",
        "  if Config.load_model:\r\n",
        "    load_checkpoint(torch.load(Config.save_filename), model, optimizer)\r\n",
        "  \r\n",
        "  sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\r\n",
        "\r\n",
        "  for epoch in range(Config.num_epochs):\r\n",
        "    print(f'Epoch[{epoch} / {Config.num_epochs}]')\r\n",
        "\r\n",
        "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\r\n",
        "    save_checkpoint(checkpoint, Config.save_filename)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    translated_sentence = translate_sentence(model, sentence, german, english, Config.device, max_length=50)\r\n",
        "\r\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    losses = []\r\n",
        "\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "      inp_data = batch.src.to(Config.device)\r\n",
        "      target = batch.trg.to(Config.device)\r\n",
        "      \r\n",
        "      output = model(inp_data, target)\r\n",
        "      # output shape: (target_len, batch_size, output_dim)\r\n",
        "      \r\n",
        "      output = output[1:].reshape(-1, output.shape[2])\r\n",
        "      target = target[1:].reshape(-1)\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss = criterion(output, target)\r\n",
        "      losses.append(loss)\r\n",
        "      loss.backward()\r\n",
        "\r\n",
        "      #to avoid exploding gradients\r\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "    print(f\"Loss: {sum(losses)/len(losses)}\")\r\n",
        "\r\n",
        "    if epoch % 5 == 0:\r\n",
        "      score = evaluation_fn(test_data[1:1000], model, german, english, Config.device)\r\n",
        "      print(f\"Bleu score {score*100:.2f}\")\r\n",
        "\r\n",
        "  score = evaluation_fn(test_data, model, german, english, Config.device)\r\n",
        "  print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8pvCVd3iuZN",
        "outputId": "607c5100-0b86-4dcf-965b-fb54ee39d232"
      },
      "source": [
        "train_fn(train_data, validation_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['rafting', 'adjacent', 'adjacent', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate', 'karate']\n",
            "Loss: 4.812053680419922\n",
            "Bleu score 4.24\n",
            "Epoch[1 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'young', 'is', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<eos>']\n",
            "Loss: 4.258140563964844\n",
            "Epoch[2 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', '<unk>', 'player', 'with', 'a', '<unk>', 'is', 'is', 'to', 'the', 'the', 'in', 'the', '.', '.', '<eos>']\n",
            "Loss: 3.797159433364868\n",
            "Epoch[3 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', '<unk>', 'with', 'a', '<unk>', 'is', 'is', 'to', 'a', 'a', 'of', 'a', '<unk>', '.', '<eos>']\n",
            "Loss: 3.5169458389282227\n",
            "Epoch[4 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'couple', 'with', 'a', '<unk>', 'with', 'a', '<unk>', 'to', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
            "Loss: 3.287140369415283\n",
            "Epoch[5 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'couple', 'with', 'a', '<unk>', '<unk>', 'is', 'next', 'to', 'a', 'large', 'large', '<unk>', '.', '<eos>']\n",
            "Loss: 3.0890700817108154\n",
            "Bleu score 15.37\n",
            "Epoch[6 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'couple', 'with', 'a', 'number', '<unk>', 'are', 'by', 'a', 'large', 'of', 'of', 'a', 'large', '.', '.', '<eos>']\n",
            "Loss: 2.9064829349517822\n",
            "Epoch[7 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', '<unk>', 'with', 'with', 'a', 'people', 'of', 'a', 'of', 'a', 'large', 'large', '.', '.', '<eos>']\n",
            "Loss: 2.7387757301330566\n",
            "Epoch[8 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'couple', 'with', 'a', 'number', 'of', 'pulled', 'by', 'a', 'large', 'of', 'of', '.', '<eos>']\n",
            "Loss: 2.5692784786224365\n",
            "Epoch[9 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'couple', 'with', 'a', 'number', 'of', 'people', 'are', 'by', 'a', 'large', 'of', '.', '<eos>']\n",
            "Loss: 2.393623113632202\n",
            "Epoch[10 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'lot', 'with', 'many', 'men', 'being', 'pulled', 'by', 'a', 'large', 'of', 'a', 'large', '.', '.', '<eos>']\n",
            "Loss: 2.2312159538269043\n",
            "Bleu score 19.72\n",
            "Epoch[11 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'a', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'large', 'of', 'a', '.', '.', '<eos>']\n",
            "Loss: 2.0609652996063232\n",
            "Epoch[12 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'a', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'large', 'by', 'a', 'large', '.', '.', '<eos>']\n",
            "Loss: 1.8794149160385132\n",
            "Epoch[13 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'by', 'a', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 1.722611904144287\n",
            "Epoch[14 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'a', 'by', 'a', 'large', 'large', '.', '.', '<eos>']\n",
            "Loss: 1.5549782514572144\n",
            "Epoch[15 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'four', 'men', 'pulled', 'pulled', 'pulled', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 1.3980896472930908\n",
            "Bleu score 20.71\n",
            "Epoch[16 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'four', 'men', 'are', 'pulled', 'to', 'a', 'a', 'large', 'by', 'a', 'large', 'horses', '.', '<eos>']\n",
            "Loss: 1.2488658428192139\n",
            "Epoch[17 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'is', 'pulled', 'by', 'a', 'a', 'a', 'large', 'large', 'boat', '.', '<eos>']\n",
            "Loss: 1.1149029731750488\n",
            "Epoch[18 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.9829045534133911\n",
            "Epoch[19 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'to', 'pulled', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.8685306906700134\n",
            "Epoch[20 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'that', 'is', 'being', 'pulled', 'to', 'pulled', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.755672037601471\n",
            "Bleu score 21.60\n",
            "Epoch[21 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'with', 'several', 'men', 'is', 'being', 'pulled', 'by', 'a', 'beach', 'by', 'by', 'large', 'sand', '.', '<eos>']\n",
            "Loss: 0.658515453338623\n",
            "Epoch[22 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'horses', 'of', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.5746322870254517\n",
            "Epoch[23 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'pulled', 'by', 'a', 'large', 'of', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.4989473223686218\n",
            "Epoch[24 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', \"'s\", 'men', 'is', 'being', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'group', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.43852728605270386\n",
            "Epoch[25 / 30]\n",
            "----------- Saving checkpoint -----------\n",
            "Translated example sentence: \n",
            " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'to', 'shore', 'by', 'a', 'large', 'bull', 'of', 'horses', '.', '<eos>']\n",
            "Loss: 0.38408949971199036\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}