# Sequence To Sequence with Attention

## Related Papers

- Sequence to Sequence Learning with Neural Models (Ilya Sutskever, Oriol Vinyals, Quoc V. Le): https://arxiv.org/abs/1409.3215
- Effective Approaches to Attention-based Neural Machine Translation (Minh-Thang Luong, Hieu Pham, Christopher D. Manning): https://arxiv.org/abs/1508.04025
- Attention Is All You Need (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin) https://arxiv.org/abs/1706.03762

## Source Material
- Stanford CS224n classes and homeworks available at: http://web.stanford.edu/class/cs224n/
- Aladin Persson's github https://github.com/aladdinpersson and youtube channel https://www.youtube.com/channel/UCkzW5JSFwvKRjXABI-UTAkQ

## Sequence To Sequence model architecture
![SeqToSeq](https://github.com/shawn-lab-ml/projects/blob/master/nlp/SeqToSeq_MT/sequence-to-sequence-model-with-attention.png)
source: https://www.researchgate.net/publication/321210603_Using_stochastic_computation_graphs_formalism_for_optimization_of_sequence-to-sequence_model
